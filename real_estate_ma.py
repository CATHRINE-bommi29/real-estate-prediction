# -*- coding: utf-8 -*-
"""real estate_MA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QfjMWYHrAFtHFufEVPmgEqP9UvzLrskT
"""

# Commented out IPython magic to ensure Python compatibility.

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import matplotlib

from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso, Ridge, LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import KFold
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score


from ipywidgets import widgets
from IPython.display import display
import warnings
warnings.filterwarnings('ignore')
matplotlib.rcParams["figure.figsize"] = (16,8)

"""1| Load Dataset"""

df = pd.read_csv('/content/Bengaluru_House_Data (1).csv')

df.head()

"""2| Dataset Exploration"""

df.shape

area_types_count = df['area_type'].value_counts(normalize=True)

fig, ax = plt.subplots(figsize=(5, 5))
area_types_count.plot(kind='bar', ax=ax)
plt.ylabel('Area Type')
plt.xlabel('Frequency')
plt.title('Area type counts');

fig, ax = plt.subplots(figsize=(5, 5))
(
    df.sort_values(by='price', ascending=False).head(20)
    .groupby('area_type')['area_type'].count()
    .plot(kind='bar', ax=ax)
)
plt.xlabel("Area Type")
plt.ylabel("Frequency")
plt.title("Top 20 expensive houses count (Area Type representation)")
plt.legend();

fig, ax = plt.subplots(figsize=(5, 5))
(
    df.sort_values(by='price', ascending=True).head(20)
    .groupby('area_type')['area_type'].count()
    .plot(kind='bar', ax=ax)
)
plt.xlabel("Area Type")
plt.ylabel("Frequency")
plt.title("20 Least expensive houses count (Area Type representation)")
plt.legend();

df['availability'].value_counts()

df['size'].unique()

"""3| Data Cleaning and Dataset Transformation"""

df.isnull().sum()

numerical_features = df.select_dtypes(include=[int, float])
numerical_features.isnull().sum() > 0

"""I can fill missing values by the average as integer for these numerical features. Categorical features which are null will be deleted (dropped) from the dataset"""

numerical_features.apply(lambda x: round(np.mean(x)))

df['bath'].fillna(3, inplace=True)
df['balcony'].fillna(2, inplace=True)
df.isnull().sum()

df.shape

"""Nothing is yet removed from dataset"""

df['size'].unique()

df['bhk'] = df['size'].str.split(' ', expand=True)[0]
df['bhk'] = df['bhk'].astype(float)
df.head()

"""Now that becase bhk is exracted frin size, we can even remove this feature from dataset


"""

df = df.drop(columns='size')
df.head()

df['bhk'].unique()

"""It is time to change bhk feature datatype from object to floaT"""

df['bhk'].astype(float)

"""We have to create new dataset that don't take into consideration entries where bhk in null"""

df_bhk = df[~df['bhk'].isnull()]
df_bhk.head()

df_bhk.isnull().sum()

"""Let me change the total_sqft data type to float"""

df_bhk[df_bhk['total_sqft'].apply(lambda x: '-' in x )].head()

# We will first need to change data type from string to float values
def convert_to_float (num):
    tokenized = num.split('-')
    if len(tokenized) == 2:
        return (float(tokenized[0]) + float(tokenized[1])) / 2
    try:
        return float(num)
    except Exception as e:
        return None
df_bhk['total_sqft'] = df_bhk['total_sqft'].apply(convert_to_float)

"""Let's check for 30 index which was initially has dirty total_sqft values"""

df_bhk.iloc[30]

"""As of above, you can see that total_sqft is now 2475.0


When buying new house there is many features that is considered such as its location, dimension(square feet), number of rooms, architecture, bathrooms, etc.


Now we are going to look for those features against house price to check if there is a relationship.


Location vs Price
"""

df_loc =df_bhk.copy()
df_loc.head()

df_loc['location'].value_counts()

"""It seems like location can cause high cardinality issues. It is time to check for under represented location and group them


"""

less_than_10 = df_loc['location'].value_counts() < 10
less_than_10 = less_than_10[less_than_10 ]
less_than_10

df_loc['location'] = df_loc['location'].apply(lambda x: 'Other' if x in less_than_10 else x )
df_loc['location'].nunique()

price_mean = df_loc['price'].mean()
plt.figure(figsize=(10, 6))
sns.scatterplot(x='location', y='price', data=df_loc, alpha=.5)
plt.axhline(price_mean, color='red', linestyle='--', label=f'Price Mean (y = {price_mean:.2f})')
plt.ylabel('House Price')
plt.xlabel('House Location')
plt.xticks([])
plt.legend();

"""
Most of location, houses price is less than 1000. What does this tell us?? There should be potential outliers.


Another takeway is that mean is about 112. It seems that those outliers are skewing our data.

Maybe we can exclude house with price that is above 1000. But first let me see statistical information
"""

df_loc['price'].describe()

"""Price vs Square FeeT"""

df_loc['price_per_sqft']  = round(df_loc['price']*100_000/df_loc['total_sqft'], 2)
df_loc.head()

"""

Let's use price per square feets to check how price is spread accross the dataset

"""

# Function that formats y axis to use M for millions and K for thousands
def ticker_values_formatter(value, ticker_num):
    if value >= 1e6:
        return f'{int(value/1e6)}M'
    elif value >=1e3:
        return f'{int(value/1e3)}K'
    else:
        return f'{int(value)}'

price_per_sqft_mean = df_loc['price_per_sqft'].mean()
plt.figure(figsize=(10, 6))
sns.scatterplot(x='location', y='price_per_sqft', data=df_loc, alpha=.5)
plt.axhline(price_per_sqft_mean,
            color='red', linestyle='--',
            label=f'Price Mean (y = {price_per_sqft_mean:.2f})'
           )
plt.ylabel('House Price Per Square Feets')
plt.xlabel('House Location')
plt.xticks([])
plt.gca().yaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(ticker_values_formatter))
plt.gca().yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(1e6))
plt.legend();

"""REMOVING OUTLIERS"""

df_no_extreme = df_loc.copy()
df_no_extreme.shape

df_cleaned_1 = df_no_extreme[df_no_extreme['price_per_sqft'] < 100_000]
df_cleaned_1.shape

price_per_sqft_mean = df_cleaned_1['price_per_sqft'].mean()
plt.figure(figsize=(10, 6))
sns.scatterplot(x='location', y='price_per_sqft', data=df_cleaned_1, alpha=.5)
plt.axhline(price_per_sqft_mean,
            color='red', linestyle='--',
            label=f'Price Mean (y = {price_per_sqft_mean:.2f})'
           )
plt.ylabel('House Price Per Square Feets')
plt.xlabel('House Location')
plt.xticks([])
plt.gca().yaxis.set_major_formatter(matplotlib.ticker.FuncFormatter(ticker_values_formatter))
plt.gca().yaxis.set_major_locator(matplotlib.ticker.MultipleLocator(100_000))
plt.legend();

"""I'm gonna remove values that fall bellow 1% and 99% percentiles"""

perc_99 = df_cleaned_1['price_per_sqft'] < np.percentile(df_cleaned_1['price_per_sqft'], 90)
perc_10 = df_cleaned_1['price_per_sqft'] > np.percentile(df_cleaned_1['price_per_sqft'], 10)
df_cleaned_2 = df_cleaned_1[perc_10 & perc_99]
df_cleaned_2.shape

plt.hist(df_cleaned_2['price_per_sqft'])

"""We can even go deeper to what effect bhk can have to price of the house. However, I am going straing to encoding categorical features that I'll be using for model training



4 | Building and Training a Model


It's time to encode categorical features because machine learning algorithms don't cope with texts
"""

categorical_feat = df_cleaned_2.select_dtypes("object").drop(['society', 'area_type', 'availability'], axis='columns').columns.to_list()
categorical_feat

dumies = pd.get_dummies(df_cleaned_2[categorical_feat])
dumies.head(3)

df_cleaned_3 = (
    pd.concat(
        [
            df_cleaned_2.drop(columns=['society', 'area_type', 'availability']),
            dumies.drop('location_Other',axis = 'columns')
        ],
        axis='columns')
)
df_cleaned_3.head()

df_ready = df_cleaned_3.drop(categorical_feat, axis='columns')
df_ready.head()

df_ready.select_dtypes(include=[int, float]).corr()

fig, ax = plt.subplots(figsize=(10, 5))
sns.heatmap(df_ready.select_dtypes(include=[int, float]).corr(), ax=ax);

X = df_ready.drop('price', axis='columns')
X.head()

y = df_ready['price']
y.head()

"""Building baseline"""

y_mean = round(np.mean(y), 2)
y_baseline = [y_mean] * len(y)

"""
Now it is time to split dataset for training and testing datasets

Train Test Split
"""

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=10)
mean_absolute_error(y, y_baseline)

"""Fitting Linear Regression Model


Now we have to fit the model on algorithm

"""

reg_model=  LinearRegression()
#fitting the model
reg_model.fit(X_train, y_train)

#Check check score on training data set
train_y_pred_score = round(reg_model.score(X_train, y_train), 2)
train_y_pred_score

#Check check score on training data set
test_y_pred_score = round(reg_model.score(X_test, y_test), 2)
test_y_pred_score

reg_model.intercept_

reg_model.coef_[:5]

y_pred = pd.Series(reg_model.predict(X_train)).apply(lambda x: round(x, 2))
print(pd.Series(y_pred[:5]))
pd.Series(y_train[:5])

round(mean_absolute_error(y_pred, y_train), 2)

residuals = y_pred - y_train
fig, ax= plt.subplots(figsize=(10, 5))
sns.histplot(residuals, kde=True, bins=30, ax=ax)
plt.title("Model Prediction Residuals distribution");

# For linear regression coefficients
coefficients = pd.Series(reg_model.coef_)
coefficients.sort_values().plot(kind='barh', figsize=(8, 6))
plt.title('Feature Importance')
plt.yticks([])
plt.show()

"""Let's use cross validation to check model performance. But later on, I will use Grid search validation which allows to test more than one machine learning model at time


"""

cv_1 = KFold(n_splits=5, shuffle=False, random_state=None)
cv_2 = KFold(n_splits=5, shuffle=True, random_state=42)
print(f'Without shuffling: {cross_val_score(LinearRegression(), X, y, cv=cv_1)}')
print(f'With shuffling: {cross_val_score(LinearRegression(), X, y, cv=cv_2)}')

#Using ShuffleSplit method
cv_s = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)
cross_val_score(LinearRegression(), X, y, cv=cv_s)

"""5 | Model Evaluation

As owner of the notebook I copied, I am going to use Grid Search technique to evaluate model on different machine learning algorithms to check which performs better.

I am going to test Ridge, Lasso, LinearRegression, and DecisionTreeRegressor.
"""

def grid_search_func(X,y):
    """
    Function: grid_search_func
    This function utilize GridSearchCV to use different machine learning algorithms to check algorithm which works better.
    Parameters:
        X: Predictors
        y: target variable (to be predicted)
    Return:
        Function return data frame with three columns/feaures which are 'model', best_score, best_params'
    """
    alg_config = {
        'LinearRegression' : {
            'model' : LinearRegression(),
            'params' : {
            }
        },
        'Ridge': {
            'model': Ridge(),
            'params':{
                'alpha': [1, 2]
            }
        },
        'Lasso' : {
            'model' : Lasso(),
            'params' : {
                'alpha' : [1,2],
                'selection' : ['random', 'cyclic']
            }
        },
        'decision_tree' : {
            'model' : DecisionTreeRegressor(),
            'params' : {
                'criterion' : ['squared_error', 'friedman_mse'],
                'splitter' : ['best', 'random']
            }
        }
    }

    scores = []
    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)

    for model, config in alg_config.items():
        grid_search = GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)
        print(grid_search)
        grid_search.fit(X,y)
        print('finished fitting the moel')
        scores.append({
            'model': model,
            'best_score': grid_search.best_score_,
            'best_params': grid_search.best_params_
        })
    return pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])

#Testing function
model_perfomance = grid_search_func(X, y)
model_perfomance

model_perfomance

"""5| COMMUNICATION


Building functin that can recieve location, square feets, bath and bhk (bedrooms, hall, kitchen) and return prediction
"""

def predict_price(location):
    """
    Generate prediction of one location
    Returns turple
        index 0: prediction
        index 1: Real Value
    """
    try:
        index = np.where(X.columns==location)[0][0]
        y_pred = round(reg_model.predict(X[X.index ==index])[0], 2)
        return(f'Prediction: {y_pred}, Real value: {y[y.index ==index].to_list()[0]}')
    except Exception as e:
        return None
#Initialize output area
output = widgets.Output()
def on_dropdown_value_change(change):
    location = change['new']
    with output:
        output.clear_output()
        prediction = predict_price(location)
        output.append_display_data(display_object=prediction)
        f"Prediction for {location} is: {prediction} Price"
options = [location for location in X.columns.to_list() if location.startswith('location')]
dropdown = widgets.Dropdown(
    options=options,
    value=options[49],
    description="Location Dropdown"
)
display(dropdown, output)
dropdown.observe(on_dropdown_value_change, names='value')